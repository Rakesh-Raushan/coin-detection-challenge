"""
Evaluate a trained model using the existing evaluation framework.

Thin wrapper that configures EvalConfig to point at a finetuned model
and delegates to the project's evaluation module.

Supports a --test flag that automatically points at the held-out test set
(COCO format) generated by prepare_dataset.py, ensuring final metrics
are reported on data never seen during training or checkpoint selection.
"""

import sys
import argparse
from pathlib import Path


# Add project root to path so we can import the evaluation module
PROJECT_ROOT = Path(__file__).parent.parent.parent.resolve()
sys.path.insert(0, str(PROJECT_ROOT))

from evaluation import Evaluator, EvalConfig


TRAINING_ROOT = Path(__file__).parent.parent.resolve()

# Default test set paths (generated by prepare_dataset.py)
DEFAULT_TEST_IMAGES = TRAINING_ROOT / "data" / "processed" / "test" / "images"
DEFAULT_TEST_ANNOTATIONS = TRAINING_ROOT / "data" / "processed" / "test" / "_annotations.coco.json"


def evaluate(
    model_path: Path,
    use_test_set: bool = False,
    data_dir: Path | None = None,
    annotations_path: Path | None = None,
    conf_threshold: float = 0.25,
):
    """
    Run evaluation on a trained model.

    Args:
        model_path: Path to the .pt model file.
        use_test_set: If True, evaluate on the held-out test split.
        data_dir: Override image directory (ignored if use_test_set=True).
        annotations_path: Override annotations path (ignored if use_test_set=True).
        conf_threshold: Confidence threshold for detections.
    """
    config = EvalConfig()

    # Override model path
    config.MODEL_PATH = Path(model_path)
    config.CONF_THRESHOLD = conf_threshold

    if use_test_set:
        # Point at held-out test set
        if not DEFAULT_TEST_ANNOTATIONS.exists():
            raise FileNotFoundError(
                f"Test annotations not found at {DEFAULT_TEST_ANNOTATIONS}.\n"
                f"Run prepare_dataset.py first to generate the test split."
            )
        config.DATA_DIR = DEFAULT_TEST_IMAGES
        config.ANNOTATIONS_PATH = DEFAULT_TEST_ANNOTATIONS
        print("=" * 60)
        print("EVALUATING ON HELD-OUT TEST SET")
        print("  (data never seen during training or checkpoint selection)")
        print("=" * 60)
    else:
        # Manual overrides
        if data_dir:
            config.DATA_DIR = Path(data_dir)
        if annotations_path:
            config.ANNOTATIONS_PATH = Path(annotations_path)

    print(f"Model: {config.MODEL_PATH}")
    print(f"Dataset: {config.DATA_DIR}")
    print(f"Annotations: {config.ANNOTATIONS_PATH}")
    print(f"Confidence threshold: {config.CONF_THRESHOLD}")
    print()

    evaluator = Evaluator(config)
    artifact_paths = evaluator.run()

    print(f"\nArtifacts saved to: {config.OUTPUT_DIR}")
    for name, path in artifact_paths.items():
        print(f"  {name}: {path.name}")

    return artifact_paths


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Evaluate a finetuned model using the project evaluation framework"
    )
    parser.add_argument(
        "model_path",
        type=Path,
        help="Path to the model .pt file (e.g., experiments/<exp>/train/weights/best.pt)",
    )
    parser.add_argument(
        "--test",
        action="store_true",
        default=False,
        help="Evaluate on held-out test set (recommended for final reporting)",
    )
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=None,
        help="Override dataset images directory (ignored if --test is used)",
    )
    parser.add_argument(
        "--annotations",
        type=Path,
        default=None,
        help="Override annotations file path (ignored if --test is used)",
    )
    parser.add_argument(
        "--conf",
        type=float,
        default=0.25,
        help="Confidence threshold (default: 0.25)",
    )
    args = parser.parse_args()

    evaluate(
        model_path=args.model_path,
        use_test_set=args.test,
        data_dir=args.data_dir,
        annotations_path=args.annotations,
        conf_threshold=args.conf,
    )
